{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.freelancer.com'\n",
    "access_token=\"<access_token>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome(executable_path = \"chromedriver-win64/chromedriver.exe\")\n",
    "\n",
    "# url = \"https://www.freelancer.com/job/\"\n",
    "\n",
    "# driver.get(url)\n",
    "# time.sleep(5)\n",
    "# # Lấy HTML sau khi trang đã tải xong\n",
    "# html_content = driver.page_source\n",
    "# # Phân tích HTML bằng BeautifulSoup\n",
    "# # soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# driver.quit()\n",
    "# file_path = \"C:/Users/Admin/OneDrive - VNU-HCMUS/Documents/K1N4/UD_PTDLTM/freelancer_jobs.html\"\n",
    "\n",
    "# with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "#     file.write(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1 = web, 2 = Mobile, 6 = Data,  7 = AI\n",
    "\n",
    "\n",
    "# #  Categories to target\n",
    "# categories = [2, 3, 6, 7]\n",
    "\n",
    "# # Create a dictionary to hold lists of tuples (href, job_count) for each category\n",
    "# category_links = {category_id: [] for category_id in categories}\n",
    "\n",
    "# # Parse the HTML\n",
    "# soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# # Iterate over the categories\n",
    "# for category_id in categories:\n",
    "#     category_section = soup.find('section', id=f'category-{category_id}')\n",
    "#     if category_section:\n",
    "#         # Find all list items (jobs) in the category\n",
    "#         job_items = category_section.find_all('li')\n",
    "        \n",
    "#         for job_item in job_items:\n",
    "#             # Find the link inside the job item\n",
    "#             job_link = job_item.find('a', class_='PageJob-category-link')\n",
    "#             if job_link:\n",
    "#                 # Extract the job count using regex\n",
    "#                 job_text = job_link.get_text(strip=True)\n",
    "#                 job_count = re.search(r'\\((\\d+)\\)', job_text)\n",
    "                \n",
    "#                 if job_count and int(job_count.group(1)) > 0:\n",
    "#                     job_href = job_link['href']\n",
    "#                     # Save href and job_count as a tuple (href, job_count)\n",
    "#                     category_links[category_id].append((job_href, int(job_count.group(1))))\n",
    "\n",
    "# # Print the lists for each category\n",
    "# for category_id, href_list in  category_links.items():\n",
    "#     print(f'Category {category_id} links: {href_list}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Save category_links to a JSON file\n",
    "# with open('category_links.json', 'w', encoding='utf-8') as json_file:\n",
    "#     json.dump(category_links, json_file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_of_each_list = \"https://www.freelancer.com/jobs/dot-net/1/\"\n",
    "\n",
    "# headers = {\n",
    "#         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "#     }\n",
    "\n",
    "# # Gửi yêu cầu HTTP GET đến URL với headers\n",
    "# response = requests.get(url_of_each_list, headers=headers)\n",
    "\n",
    "# # Kiểm tra xem yêu cầu có thành công hay không\n",
    "# if response.status_code == 200:\n",
    "#     html_content = response.text\n",
    "#     file_path = \"C:/Users/Admin/OneDrive - VNU-HCMUS/Documents/K1N4/UD_PTDLTM/joblist.html\"\n",
    "#     with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "#         file.write(html_content)\n",
    "# else:\n",
    "#     print(\"failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # - với mỗi category 2,3,6,7 trong json\n",
    "# #     - lấy ra link và số lượng job\n",
    "# #     - n_page = số lượng job chia nguyên cho 50\n",
    "# #     - với mỗi (int)page_i trong n_page\n",
    "# #     - url = f\"https://www.freelancer.com + {link} + {page_i}\n",
    "# import math\n",
    "\n",
    "# with open('category_links.json', 'r', encoding='utf-8') as json_file:\n",
    "#     category_links = json.load(json_file)\n",
    "\n",
    "\n",
    "# # Dictionary to hold all the URLs for each category\n",
    "# category_urls = {}\n",
    "\n",
    "# # Iterate over each category (2, 3, 6, 7)\n",
    "# for category_id, jobs in category_links.items():\n",
    "#     urls = []  # List to store URLs for the current category\n",
    "    \n",
    "#     for job in jobs:\n",
    "#         link, job_count = job\n",
    "        \n",
    "#         # Calculate the number of pages\n",
    "#         n_page = math.ceil(job_count / 50)  # Round up to cover all jobs\n",
    "        \n",
    "#         # Generate the URLs for each page\n",
    "#         for page_i in range(1, n_page + 1):\n",
    "#             url = f\"https://www.freelancer.com{link}?page={page_i}\"\n",
    "#             urls.append(url)  # Add the URL to the list\n",
    "    \n",
    "#     # Store the URLs list in the dictionary\n",
    "#     category_urls[category_id] = urls\n",
    "\n",
    "# # Save the category URLs to a new JSON file\n",
    "# with open('list_jobs_URL_of_each_category.json', 'w', encoding='utf-8') as json_file:\n",
    "#     json.dump(category_urls, json_file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of Job URLs from the url page in the list_jobs_URL_of_each_category.json\n",
    "def get_job_atrributes(url):\n",
    "\n",
    "    soup=BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "\n",
    "    # Get the seo_url through class=\"JobSearchCard-primary-heading\" html\n",
    "    anchor_tag = soup.find_all('a', class_='JobSearchCard-primary-heading-link')\n",
    "\n",
    "    # Get the 'href' attribute\n",
    "    href_list = [tag['href'] for tag in anchor_tag]\n",
    "\n",
    "    return href_list\n",
    "\n",
    "#Read file json\n",
    "def read_json_list_jobs(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "# From list of job in each page, get seo url of each job, and each contest\n",
    "def get_seo_url_json_list_jobs(list_url):\n",
    "    contests_seo_urls=[]\n",
    "    projects_seo_urls=[]\n",
    "    for url in list_url:\n",
    "        \n",
    "        seo_urls=get_job_atrributes(url)\n",
    "        for seo_url in seo_urls:\n",
    "            \n",
    "            #If have 'contest' in url, append to contest list\n",
    "            if seo_url[0:9]=='/contest/':\n",
    "                contests_seo_urls.append(seo_url)\n",
    "            else :\n",
    "                projects_seo_urls.append(seo_url)\n",
    "                \n",
    "    #Remove \"/projects/\" in url\n",
    "    projects_seo_urls=[url.replace(\"/projects/\",\"\") for url in projects_seo_urls]\n",
    "    \n",
    "    #Apply regex to get the contest id contain 7 digits\n",
    "    contests_id=[re.search(r'\\d{7}',url).group(0) for url in contests_seo_urls]\n",
    "    \n",
    "    return projects_seo_urls, contests_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_api_url_request(p_base_url,p_params):\n",
    "    \n",
    "    # Encode parameters\n",
    "    encoded_params = urllib.parse.urlencode(p_params, doseq=True)\n",
    "\n",
    "    # Construct the full URL\n",
    "    url = f'{p_base_url}?{encoded_params}'\n",
    "\n",
    "    # Headers for authorization\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {access_token}'\n",
    "    }\n",
    "    \n",
    "    return url, headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from API, input is list of contest id, output is data response from API\n",
    "def get_contests(p_contests_id):\n",
    "    \n",
    "    # Parameters contructed as a dictionary\n",
    "    params = {\n",
    "        'contests[]': p_contests_id,\n",
    "        'job_details':True,\n",
    "    }\n",
    "    base_url = 'https://www.freelancer.com/api/contests/0.1/contests/'\n",
    "    url, headers = create_api_url_request(base_url, params)\n",
    "    try: \n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Error: {e}')\n",
    "        return None\n",
    "\n",
    "# Get data from API, input is list of project seo urls, output is data response from API\n",
    "def get_projects(p_seo_urls):\n",
    "    # Parameters contructed as a dictionary\n",
    "    params = {\n",
    "        'seo_urls[]': p_seo_urls,\n",
    "        'full_description': True,\n",
    "        'job_details': True,\n",
    "    }\n",
    "    base_url = 'https://www.freelancer.com/api/projects/0.1/projects/'\n",
    "    url, headers = create_api_url_request(base_url, params)\n",
    "    try: \n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Error: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from API, input is start_id, end_id, category (file list_jobs_URL_of_each_category), output is data response from API\n",
    "def get_jobs(p_category,p_min,p_max):\n",
    "    current_file_path=os.getcwd()\n",
    "    #json.loads(open(, \"r\", encoding=\"utf-8\").read())\n",
    "    file_path=\"/\".join((os.path.dirname(os.path.dirname(os.path.dirname(current_file_path))),\\\n",
    "                        \"data\",\"utils\",\"list_jobs_URL_of_each_category.json\"))\n",
    "    body_content=read_json_list_jobs(file_path)\n",
    "\n",
    "\n",
    "    # Get urls from p_min index to p_max index of category p_category\n",
    "    body_content=body_content[str(p_category)][p_min:p_max]\n",
    "\n",
    "    projects_seo_urls, contests_id=get_seo_url_json_list_jobs(body_content)\n",
    "\n",
    "    projects=None\n",
    "    contests=None\n",
    "    if len(projects_seo_urls)>0:\n",
    "        try:\n",
    "            projects=get_projects(projects_seo_urls)\n",
    "        except:\n",
    "            pass\n",
    "    if len(contests_id)>0:\n",
    "        try:\n",
    "            contests=get_contests(contests_id)\n",
    "        except:\n",
    "            pass\n",
    "    return projects, contests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of bider of project id from API\n",
    "def get_bid(project_id):\n",
    "    # Parameters contructed as a dictionary\n",
    "    params = {\n",
    "\n",
    "    }\n",
    "    base_url = 'https://www.freelancer.com/api/projects/0.1/projects/{}/bids/'.format(project_id)\n",
    "    url, headers = create_api_url_request(base_url, params)\n",
    "    try: \n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Error: {e}')\n",
    "        return None\n",
    "# Get list of entrants of contest id from API (Competition)\n",
    "def get_entrants(contest_ids:list):\n",
    "    # Parameters contructed as a dictionary\n",
    "    params = {\n",
    "        'contest_ids[]': contest_ids,\n",
    "    }\n",
    "    base_url = 'https://www.freelancer.com/api/contests/0.1/entrants/all/'\n",
    "    url, headers = create_api_url_request(base_url, params)\n",
    "    try: \n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Error: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data from API to dataframe\n",
    "def to_df_projects_contests(projects,contests,log_status=True):\n",
    "    df=pd.DataFrame(columns=[\"job_id\",\"owner_id\",\"type\",\"title\",\"services\",\"skills\",\"description\",\n",
    "                             \"date_posted\",\"time_remaining\",\"budget_min\",\"budget_max\",\"budget_currency\",\n",
    "                             \"exchange_rate\",\"bid_count\",\"bid_list\",\"average_bid\",\"status\",\"language\",\"url\"])\n",
    "    #Save projects to dataframe\n",
    "    try: \n",
    "        for project in projects['result']['projects']:\n",
    "            if project is None:\n",
    "                continue\n",
    "            \n",
    "            if log_status:\n",
    "                print(\"INFO: Getting project with id: \",project['id'])\n",
    "                \n",
    "            job_id=project['id']\n",
    "            owner_id=project['owner_id']\n",
    "            _type=\"Project\"\n",
    "            title=project['title']\n",
    "            services=project['seo_url'].split(\"/\")[0]\n",
    "            skills=[skill['name'] for skill in project['jobs']]\n",
    "            description=project['description']\n",
    "            date_posted=project['submitdate']\n",
    "            time_free_bids_expire=project['time_free_bids_expire']\n",
    "            budget_min=project['budget']['minimum']\n",
    "            budget_max=project['budget']['maximum']\n",
    "            budget_currency=project['currency']['code']\n",
    "            exchange_rate=project['currency']['exchange_rate']\n",
    "            bid_count=project['bid_stats']['bid_count']\n",
    "            bid_list=[bid_user['bidder_id'] for bid_user in get_bid(job_id)['result']['bids']]\n",
    "            average_bid=project['bid_stats']['bid_avg']\n",
    "            status=project['status']\n",
    "            language=project['language']\n",
    "            url=\"https://www.freelancer.com/projects\"+project['seo_url']\n",
    "            #Check if the job_id already exists in the dataframe\n",
    "            if job_id in df['job_id'].values:\n",
    "                continue\n",
    "            else:\n",
    "                new_row_df=pd.DataFrame({\"job_id\":job_id,\"owner_id\":owner_id,\"type\":_type,\"title\":title,\\\n",
    "                                        \"services\":services,\"skills\":[skills],\"description\":description,\\\n",
    "                                        \"date_posted\":date_posted,\"time_remaining\":time_free_bids_expire,\\\n",
    "                                        \"budget_min\":budget_min,\"budget_max\":budget_max,\"budget_currency\":budget_currency,\\\n",
    "                                        \"exchange_rate\":exchange_rate,\"bid_count\":bid_count,\"bid_list\":[bid_list],\"average_bid\":average_bid,\\\n",
    "                                        \"status\":status,\"language\":language,\"url\":url})\n",
    "                df=pd.concat([df,new_row_df],ignore_index=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    #Save contests to dataframe\n",
    "    try: \n",
    "        for contest in contests['result']['contests']:\n",
    "            if contest is None:\n",
    "                continue\n",
    "            if log_status:\n",
    "                print(\"INFO: Getting contest with id: \",contest['id'])\n",
    "            job_id=contest['id']\n",
    "            owner_id=contest['owner_id']\n",
    "            _type=\"Contest\"\n",
    "            title=contest['title']\n",
    "            services=None\n",
    "            skills=[skill['name'] for skill in contest['jobs']]\n",
    "            description=contest['description']\n",
    "            date_posted=contest['time_submitted']\n",
    "            time_remaining=None\n",
    "            budget_min=contest['prize']\n",
    "            budget_max=contest['prize']\n",
    "            budget_currency=contest['currency']['code']\n",
    "            exchange_rate=contest['currency']['exchange_rate']\n",
    "            bid_list=[bid_user['id'] for bid_user in get_entrants([job_id])['result']['entrants']]\n",
    "            bid_count=len(bid_list)\n",
    "            average_bid=None\n",
    "            status=contest['status']\n",
    "            language=contest['language']\n",
    "            url=\"https://www.freelancer.com/\"+contest['seo_url']\n",
    "            #Check if the job_id already exists in the dataframe\n",
    "            if job_id in df['job_id'].values:\n",
    "                continue\n",
    "            else:\n",
    "                new_row_df=pd.DataFrame({\"job_id\":job_id,\"owner_id\":owner_id,\"type\":_type,\"title\":title,\\\n",
    "                                        \"services\":services,\"skills\":[skills],\"description\":description,\\\n",
    "                                        \"date_posted\":date_posted,\"time_remaining\":time_remaining,\\\n",
    "                                        \"budget_min\":budget_min,\"budget_max\":budget_max,\"budget_currency\":budget_currency,\\\n",
    "                                        \"exchange_rate\":exchange_rate,\"bid_count\":bid_count,\"bid_list\":[bid_list],\"average_bid\":average_bid,\\\n",
    "                                        \"status\":status,\"language\":language,\"url\":url})\n",
    "                df=pd.concat([df,new_row_df],ignore_index=True)\n",
    "    except:\n",
    "        pass\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv file (Projects and Contests)\n",
    "def save_projects_contests_to_csv(df,category,p_min,p_max):\n",
    "    current_file_path=os.getcwd()\n",
    "    file_path=\"/\".join((os.path.dirname(os.path.dirname(os.path.dirname(current_file_path))),\\\n",
    "                        \"data\",\"raw\",\"freelancer_jobs\",f\"freelancer_jobs_category_{category}_{p_min}_{p_max}.csv\"))\n",
    "    df.to_csv(file_path,index=False)\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to get data projects and contests from API\n",
    "def get_data_jobs(category,p_min,p_max,log_status=True):\n",
    "    projects,contests=get_jobs(category,p_min,p_max)\n",
    "    df=to_df_projects_contests(projects,contests,log_status)\n",
    "    file_path=save_projects_contests_to_csv(df,category,p_min,p_max)\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of user (bider and entrants) from csv file\n",
    "def get_user_id(file_path):\n",
    "    df=pd.read_csv(file_path)\n",
    "    return df['bid_list'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from API, input is list of user id, output is data response from API\n",
    "def get_user(p_user_ids):\n",
    "     # Parameters contructed as a dictionary\n",
    "    params = {\n",
    "        'users[]': p_user_ids,\n",
    "        'profile_description':True,\n",
    "        'reputation'  :True,\n",
    "        'reputation_extra':True,\n",
    "    }\n",
    "    base_url = 'https://www.freelancer.com/api/users/0.1/users/'\n",
    "    url, headers = create_api_url_request(base_url, params)\n",
    "    try: \n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Error: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data from API to dataframe\n",
    "def to_df_users(data):\n",
    "    df=pd.DataFrame(columns=[\"user_id\",\"username\",\"region\",\"overview\",\"ratings\",\"completed_ratings\",\"url\"])\n",
    "    for user in list(data['result']['users'].values()):\n",
    "        user_id=user['id']\n",
    "        username=user['username']\n",
    "        region=user['location']['country']['name']\n",
    "        overview=user['profile_description']\n",
    "        ratings=user['reputation']['entire_history']['overall']\n",
    "        completed_ratings=user['reputation']['entire_history']['completion_rate']\n",
    "        url=\"https://www.freelancer.com/u/\"+username\n",
    "        if user_id in df['user_id'].values:\n",
    "            continue\n",
    "        else:\n",
    "            new_row_df=pd.DataFrame({\"user_id\":user_id,\"username\":username,\"region\":region,\\\n",
    "                                    \"overview\":overview,\"ratings\":ratings,\"completed_ratings\":completed_ratings,\\\n",
    "                                    \"url\":url},index=[0])\n",
    "        df=pd.concat([df,new_row_df],ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv file (Users)\n",
    "def save_users_to_csv(df,category,p_min,p_max):\n",
    "    current_file_path=os.getcwd()\n",
    "    file_path=\"/\".join((os.path.dirname(os.path.dirname(os.path.dirname(current_file_path))),\\\n",
    "                        \"data\",\"raw\",\"freelancer_users\",f\"freelancer_users_category_{category}_{p_min}_{p_max}.csv\"))\n",
    "    df.to_csv(file_path,index=False)\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to get data users from API\n",
    "def get_data_users(file_jobs_path,p_categories,p_min,p_max,log_status=True):\n",
    "    list_user_ids=get_user_id(file_jobs_path)\n",
    "    df=pd.DataFrame(columns=[\"user_id\",\"username\",\"region\",\"overview\",\"ratings\",\"completed_ratings\",\"url\"])\n",
    "    for i,user_ids in enumerate(list_user_ids):\n",
    "        try:\n",
    "            user_ids=eval(user_ids)\n",
    "            if log_status:\n",
    "                print(\"INFO: Getting user info with batch: \",i)\n",
    "            data=get_user(user_ids)\n",
    "            new_df=to_df_users(data)\n",
    "            df=pd.concat([df,new_df],ignore_index=True)\n",
    "        except:\n",
    "            print('except')\n",
    "            pass\n",
    "        time.sleep(1)\n",
    "    file_path=save_users_to_csv(df,p_categories,p_min,p_max)\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 400 Client Error: Bad Request for url: https://www.freelancer.com/api/users/0.1/users/?profile_description=True&reputation=True&reputation_extra=True\n",
      "except\n",
      "Error: 400 Client Error: Bad Request for url: https://www.freelancer.com/api/users/0.1/users/?profile_description=True&reputation=True&reputation_extra=True\n",
      "except\n",
      "Error: 400 Client Error: Bad Request for url: https://www.freelancer.com/api/users/0.1/users/?profile_description=True&reputation=True&reputation_extra=True\n",
      "except\n"
     ]
    }
   ],
   "source": [
    "#Choose category, start_index, end_index in list_jobs_URL_of_each_category.json\n",
    "category=2\n",
    "start_index=0\n",
    "end_index=3\n",
    "\n",
    "# Get data from API, if don't want to print log_status, set log_status=False\n",
    "file_jobs_path=get_data_jobs(category,start_index,end_index,log_status=False)\n",
    "file_users_path=get_data_users(file_jobs_path,category,start_index,end_index,log_status=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
